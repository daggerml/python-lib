#!/usr/bin/env python3
import sys
from datetime import datetime, timezone
from uuid import uuid4

import boto3

import daggerml as dml

BUCKET = 'dml-test-misc2'
PREFIX = 'batch'
STACK_NAME = 'test-batch'
JOB_NAME_PREFIX = 'dml-misc'

def now():
    return datetime.now().astimezone(timezone.utc)

if __name__ == '__main__':
    print('hello world!', file=sys.stderr)
    _id = f'{now():%Y-%m-%d-%H-%M}-{uuid4().hex}'
    cf = boto3.client('cloudformation')
    s3 = boto3.client('s3')
    input_key = f'{_id}/input.json'
    output_key = f'{_id}/output.json'
    s3.put_object(Bucket=BUCKET, Key=input_key, Body=sys.stdin.read().encode())
    stack = cf.describe_stacks(StackName=STACK_NAME)['Stacks'][0]
    outputs = {output['OutputKey']: output['OutputValue'] for output in stack['Outputs']}

    # Submit the job
    response = batch_client.submit_job(
        jobName=f'{JOB_NAME_PREFIX}-{_id}',
        jobQueue=outputs['JobQueue'],
        jobDefinition=outputs['JobDefinition'],
        containerOverrides={
            'command': ["python3", "-m", "daggerml", "run", f's3://{BUCKET}/{input_key}', f's3://{BUCKET}/{output_key}']
        }
    )
    # Print the job ID
    print(f"Job submitted! Job ID: {response['jobId']}")
    
    with dml.Api(initialize=True) as api:
        dag = dml.new('exec', 'executing dag', dump=sys.stdin.read(), api_flags=api.flags)
        resp = dag.commit(dag.put(dag.expr))
        print(dag.dump(resp))
